В этом посте я хочу рассказать о "логике" нейросетей. Я надеюсь, это поможет начинающим лучше понять, что могут нейронные сети. Для этого мы попробуем посмотреть, как они справляются с некоторыми модельными задачами. Примеры кода будут приводиться на python
с использованием библиотеки keras. 

<b>Задача 1.</b> Начнём с простого. Построим нейронную сеть, аппроксимирующую синус. 

<source lang="python">
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense


def get_X_y(n):
    X = np.random.uniform(0, np.pi, n)
    y = np.sin(X)
    return X, y


n = 40
X, y = get_X_y(n)
print("X shape:", X.shape)

model = Sequential()
model.add(Dense(6, input_dim=1, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])

model.fit(X, y, epochs=1000, batch_size=4)

X_test = np.linspace(start=0, stop=np.pi, num=500)
print("X test shape:", X_test.shape)
y_test = model.predict(X_test)

font = {'weight': 'bold',
        'size': 25}

matplotlib.rc('font', **font)
axes = plt.gca()
axes.set_ylim(0, 1)
plt.plot(X_test, y_test, c='green', marker='o', markersize=5)
plt.title("Sinus approximated by neural network")
plt.yticks(np.arange(0, 1, 0.1))
plt.grid()
plt.show()
</source>

 Получаем следующй график: 

<img src="https://habrastorage.org/webt/t3/xv/_o/t3xv_ocq-o9m8yupmxxdrfxqgai.png" width="500" height="500"/>

Как видим, нейронная сеть успешно справилась с задачей аппроксимации несложной функции. 

<b>Задача 2.</b> Посмотрим, как нейросеть справится с более сложной задачей. Будем подавать на вход значения x, равномерно распределенные на отрезке [0, 1], а y будем задавать случайно: при x < 0.6 y будет случайной величиной, принимающей значений 0 с вероятностью 0.75  и 1 с вероятностью 0.25 (то есть биномиальной случайной величиной с p=0.25). При x > 0.6 y будет случайной величиной, принимающей значение 0 с вероятностью 0.3 и значение 1 с вероятностью 0.7. В качестве оптимизируемой функции возьмем среднеквадратическую ошибку. 

<source lang="python">
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense


def get_X_y(n):
    X = np.random.uniform(0, 1, n)
    y0 = np.random.binomial(size=n, n=1, p=0.25)
    y1 = np.random.binomial(size=n, n=1, p=0.7)
    y = np.where(X < 0.6, y0, y1)
    return X, y


n_inputs = 1
n_hidden1 = 100
n_hidden2 = 50
n_outputs = 1

n = 2000

X, y = get_X_y(n)
print("X shape:", X.shape)

model = Sequential()
model.add(Dense(n_hidden1, input_dim=1, activation='relu'))
model.add(Dense(n_hidden2, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

model.fit(X, y, epochs=200, batch_size=100)

X_test = np.linspace(start=0, stop=1, num=100)
print("X test shape:", X_test.shape)
y_test = model.predict(X_test)

font = {'weight': 'bold',
        'size': 25}

matplotlib.rc('font', **font)
axes = plt.gca()
axes.set_ylim(0, 1)
plt.plot(X_test, y_test, c='green', marker='o', markersize=5)
plt.title("Binomial distribution approximated by neural network")
plt.yticks(np.arange(0, 1, 0.1))
plt.grid()
plt.show()
</source>

 Получаем следующий график аппроксимированной нейросетью функции: 

<img src="https://habrastorage.org/webt/y_/rp/lo/y_rplovhyaxioena0tb8iueed9m.png" width="500" height="500"/>

Как видим, нейронная сеть аппроксимировала математическое ожидание нашей случайной величины y. 

<b>Задача 3.</b> Теперь перейдём к предсказанию последовательностей. Будем рассматривать последовательности из 0 и 1, заданные следующим правилом: 10 членов - равновероятно 0 или 1, а одиннадцатый равен 1, если предыдущий член 0, и равновероятно 0 или 1, если предыдущий член 1. Будем генерировать такие последовательности длины 11 (10 входных членов последовательности и один, последний, мы предсказываем)  и на них обучать нашу рекуррентную нейронную сеть. А после обучения проверим, как она справляется с предсказанием на новых последовательностях (также длины 11). 

<source lang="python">
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense


def get_X_y(m, n):
    X = np.random.binomial(size=(m,n), n=1, p=0.5)
    y0 = np.ones(m)
    y1 = np.random.binomial(size=m, n=1, p=0.5)
    y = np.where(X[:, n-1]==0, y0, y1)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    return X, y


model = Sequential()
model.add(LSTM(units=50))
model.add(Dense(units=1))
model.compile(optimizer = 'adam', loss = 'mean_squared_error')

X_train, y_train = get_X_y(1000, 10)
model.fit(X_train, y_train, epochs = 20, batch_size = 32)

m_test = 12
n_test = 10
X_test, y_test = get_X_y(m_test, n_test)
y_predicted = model.predict(X_test)

for i in range(m_test):
    print("x_last=",  X_test[i, n_test-1, 0], "y_predicted=", y_predicted[i, 0])
</source>

 Посмотрим, какие прогнозы дает наша неросеть на тестируемых последовательностях (у вас результаты будут другие, поскольку здесь случайность присутствует как в выборе последовательностей, так и в обучении нейросети). 

<table>
  <tr>
    <th>Номер последовательности</th>
    <th>Предпоследний член последовательности</th>
     <th>Предсказанное значение</th>
  </tr>
  <tr>
    <td>1</td>
    <td>0</td>
    <td>0.96</td>
  </tr>
      <tr>
    <td>2</td>
    <td>0</td>
    <td>0.95</td>
  </tr>
  <tr> 
    <td>3</td>
    <td>0</td>
    <td>0.97</td>
  </tr>
  <tr> 
    <td>4</td>
    <td>0</td>
    <td>0.96</td>
  </tr>
  <tr> 
    <td>5</td>
    <td>0</td>
    <td>0.96</td>
  </tr>
  <tr> 
    <td>6</td>
    <td>1</td>
    <td>0.45</td>
  </tr>
  <tr> 
    <td>7</td>
    <td>0</td>
    <td>0.94</td>
  </tr>
  <tr> 
    <td>8</td>
    <td>1</td>
    <td>0.50</td>
  </tr>
  <tr> 
    <td>9</td>
    <td>0</td>
    <td>0.96</td>
  </tr>
  <tr> 
    <td>10</td>
    <td>1</td>
    <td>0.42</td>
  </tr>
  <tr> 
    <td>11</td>
    <td>1</td>
    <td>0.44</td>
  </tr>
  <tr> 
    <td>12</td>
    <td>0</td>
    <td>0.92</td>
  </tr>
</table>

 Как видим, если предпоследний член последовательности 0, то нейронная сеть предсказывет близкое к 1 значение, а если он равен 1, то близкое к 0.5 значение. Это близко к оптимальному прогнозу. 

<b>Задача 4.</b> Усложним задачу нейросети. Пусть всё будет как в предыдущем примере, только одиннадцатый член последовательности будет определяться не предыдущим, а вторым членом последовательности (по тому же правилу). Код здесь приводить не будем, поскольку он практически не отличается от предыдущего. Мой эксперимент показал, что неросеть все равно находит закономерность, но за больше время (пришлось использовать 100 эпох вместо 20). 

<b>Задача 5.</b> Посмотрим, как нейросеть использует имеющуюся информацию для прогноза.
Для этого проведём обучение на последовательностях длины 4. Всего у нас будет 3 разных равновероятных последовательности: 
0, 0, 1, 1
0, 1, 0, 1
0, 1, 1, 0
Таким образом, после начальной комбинации 0, 0 мы всегда встретим две единицы, после комбинации 0, 1 мы равновероятно встретим 0 или 1, зато последнее число мы будем знать наверняка. Нашу нейросеть мы теперь попросим возвращать последовательности, поставив return_sequences=True. В качестве прогнозируемых последовательностей возьмем наши же последовательности, сдвинутые на один шаг и дополненные справа нулём. Теперь мы уже можем предположить, что получится: на первом шаге нейросеть будет выдавать число, близкое к 2/3 (поскольку с вероятностью 2/3 второй член равен 1), а дальше для комбинации 0, 0 она будет выдавать два числа, близких к единице, а для 0, 1 сначала выдаст число, близкое к 0.5, а затем выдаст число, близкое к 0 или 1, в зависимости от того, получили ли мы последовательность 0, 1, 0 или 0, 1, 1. В конце нейросеть будет выдавать число, близкое к 0. Проверка с помощью следующего кода показывает, что наши предположения верны. 

<source lang="python">
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense
import random


def get_X_y(n):
    X = np.zeros((n, 4))
    z = np.array([random.randint(0, 2) for i in range(n)])
    X[z == 0, :] = [0, 0, 1, 1]
    X[z == 1, :] = [0, 1, 0, 1]
    X[z == 2, :] = [0, 1, 1, 0]
    y = np.zeros((n, 4))
    y[:, :3] = X[:, 1:]
    print(X[:5, :])
    print(y[:5, :])
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    y = np.reshape(y, (y.shape[0], y.shape[1], 1))
    return X, y


model = Sequential()
model.add(LSTM(units=20, return_sequences=True))
model.add(Dense(units=1))
model.compile(optimizer = 'adam', loss = 'mean_squared_error')

X_train, y_train = get_X_y(1000)
print(X_train.shape)
print(y_train.shape)
model.fit(X_train, y_train, epochs = 100, batch_size = 32)

X_test = np.zeros((3, 4))
X_test[0, :] = [0, 0, 1, 1]
X_test[1, :] = [0, 1, 0, 1]
X_test[2, :] = [0, 1, 1, 0]
X_test = np.reshape(X_test, (3, 4, 1))

y_predicted = model.predict(X_test)
print(y_predicted)
</source>


